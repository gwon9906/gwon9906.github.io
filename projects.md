---
layout: page
title: Projects
permalink: /projects/
---

# ğŸ”¬ IoT Communication Efficiency via BAM-based Payload Compression {#lora-communication}

## ğŸ§© What â€” Overview
Designed a **lightweight BAM-based compression model** for LoRa networks to reduce retransmissions and power drain.  
Achieved **62.5% compression** and **+14% PDR** under real-world N-LOS (2.6 km) conditions.

## ğŸ’¡ Why â€” Background
Low-power, high-loss LoRa links suffer from long payloads â†’ collisions, retransmissions, and battery drain.  
Traditional Autoencoders are too heavy for Raspberry Pi.

## âš™ï¸ How â€” Method & Implementation
- **Model**: Selected **BAM (Bidirectional Associative Memory)** for low-resource use  
- **Implementation**: Rewrote in **NumPy** for minimal dependency  
- **System**: End-to-end pipeline (capture â†’ compress â†’ transmit â†’ restore â†’ evaluate)  
- **Field Testing**: 1-month N-LOS (2.6 km) campaign, repeated TX at fixed intervals  
- **Preprocessing**: GPS integer issue fixed â†’ **MSE 0.0184 â†’ 0.0036 (80%+)**

**Stack**: Python, NumPy, Raspberry Pi, LoRa Module, Git, Linux

## ğŸš§ Challenges & Fixes
| Challenge | Solution |
|------------|-----------|
| Field variance | Extended test duration & repeated measures |
| Format inconsistency (GPS) | Preprocessing fix â†’ **MSE â†“** |
| Resource overhead | NumPy BAM enables real-time inference |

## ğŸ“ˆ Result
- **62.5% compression (32Bâ†’20B)** â†’ **+14% PDR**  
- **MSE 0.0036**, verified in N-LOS 2.6 km  
- Real-time operation on Raspberry Pi

## ğŸ” So What â€” Impact
- Demonstrated **deployable learned compression** on embedded hardware  
- Improves LoRa reliability & battery life  
- Future: PyTorch kernel & quantization for **NPU (Furiosa)** deployment

**Links**  
System: <https://github.com/4xvgal/ChirpChirp>  
Core Model: <https://github.com/gwon9906/Lightweight-MF-BAM>

---

# ğŸ“¡ Industrial Valve Flow Prediction via Encoder-LSTM {#valve-prediction}

## ğŸ§© What â€” Overview
Developed a **custom Encoder-LSTM** for industrial valve fault diagnosis.  
Achieved **98% improvement (MAPE 10 â†’ 0.188)** with robust time-series prediction.

## ğŸ’¡ Why â€” Background
Vanilla LSTM failed near zero-opening segments and was unstable against sensor outliers.

## âš™ï¸ How â€” Method & Implementation
- **Architecture**: Designed Encoder-LSTM (hierarchical feature extraction)  
- **Logic**: Added sequence reinit at valve=0  
- **Loss**: Switched to **Huber Loss** for outlier robustness  
- **Data**: Removed unnecessary normalization for low-precision floats

**Stack**: PyTorch, Python, NumPy, Pandas, Jupyter

## ğŸš§ Challenges & Fixes
| Challenge | Solution |
|------------|-----------|
| Sequence discontinuity | Sequence reinit logic solved instability |
| Outlier sensitivity | Adopted Huber Loss |
| Data precision mismatch | Simplified normalization |

## ğŸ“ˆ Result
- **MAPE 0.188**, consistent predictions across sequences  
- Enhanced stability and industrial deployability

## ğŸ” So What â€” Impact
- Validated **data-first architectural design** for robust time-series models  
- Applicable to real-world industrial fault detection  
- Future: domain-specific tuning & real-time inference

---

# ğŸ’» Ultra-Low SNR Restoration & Classification (Cascaded vs MTL) {#ultra-low-snr}

## ğŸ§© What â€” Overview
Compared **Cascaded vs Multi-Task Learning (MTL)** architectures for â€“30~â€“10 dB SNR restoration & classification.  
Implemented 6 models (BAM, CAE, U-Net Ã— Cascaded/MTL).

## ğŸ’¡ Why â€” Background
Traditional filters collapse under ultra-low SNR; the optimal **joint vs sequential** paradigm remains unclear.

## âš™ï¸ How â€” Method & Implementation
- **Backbones**: BAM / CAE / U-Net  
- **Pipelines**: Cascaded (Restorationâ†’Classification) vs MTL (Shared Encoder + Dual Decoder)  
- **Dataset**: CIFAR-10 â†’ 150 K augmented samples  
- **Noise Types**: Gaussian, Salt & Pepper, Burst  
- **SNR Levels**: â€“30, â€“25, â€“20, â€“15, â€“10 dB  

**Stack**: TensorFlow/Keras, Python, NumPy, Pandas, TensorBoard, RTX 3070 Ti

## ğŸš§ Challenges & Fixes
| Challenge | Solution |
|------------|-----------|
| Massive experiment combinations | Modular pipeline + automated logging |
| Loss imbalance | Weight grid + scheduling â†’ stable PSNRâ€“Acc trade-off |
| Unrealistic noise | Controlled SNR injection & hybrid augmentation |

## ğŸ“ˆ Result
- **U-Net** best for restoration (skip connections)  
- **MTL** better classification at higher SNR  
- **Burst noise** most challenging  

## ğŸ” So What â€” Impact
- Provides a **quantitative basis** for choosing Cascaded vs MTL under extreme noise  
- Building foundation for low-SNR inference research  
- Preparing **paper submission & PyTorch port** for real-time inference  

**Link**  
<https://github.com/gwon9906/Denoise-and-Classify>

---

# ğŸ’» Ultra-Low SNR Restoration & Classification (Cascaded vs MTL) {#ultra-low-snr}
... â† (previous content same)

---

# ğŸ§  Research Experience

**Affiliation**: AI & Embedded Systems Lab, Dong-Eui University (2024 â€“ Present)  
**Role**: Undergraduate Researcher / AI Model Optimization & IoT Systems Development

### ğŸ”¹ Focus Areas
- **IoT & Embedded AI** â€“ Low-power model optimization for edge devices  
- **Signal Processing & Time-Series Prediction** â€“ Fault detection and forecasting  
- **BAM Model Compression Research** â€“ Improving payload efficiency in noisy LoRa links  
- **Industrial AI Applications** â€“ Deployable real-world AI solutions

### ğŸ”¹ Contributions & Achievements
- Developed LoRa field test automation tools and data loggers  
- Ported TensorFlow models to NumPy for edge execution  
- Currently preparing a paper: *â€œLow-SNR Restoration & MTL Comparative Studyâ€*

---

<div style="text-align:center;margin-top:40px;">
  <a href="/index" class="btn primary" style="font-weight:bold;">â† Back to Home</a>
</div>